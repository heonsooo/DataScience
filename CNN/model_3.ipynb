{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image    \n",
    "[food] : 10000  \n",
    "[interior] : 10000  \n",
    "[exterior] : 10000   \n",
    "\n",
    "----\n",
    "데이터 30,000개 불러오기 실패\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_load_data():\n",
    "    train_images = []       \n",
    "    train_labels = []\n",
    "    shape = (300,300)  \n",
    "    train_path = './image'\n",
    "\n",
    "    for classes in ['f','i','e']:\n",
    "        for filename in os.listdir('./image'):\n",
    "            if classes == filename[0]:\n",
    "                img = cv2.imread(os.path.join(train_path,filename))\n",
    "                #print(filename)\n",
    "                # print(img)\n",
    "                # Spliting file names and storing the labels for image in list\n",
    "                train_labels.append(filename[2:0:-1])\n",
    "\n",
    "                # Resize all images to a specific shape\n",
    "                #img = cv2.resize(img,shape)\n",
    "\n",
    "                train_images.append(img)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    # Converting labels into One Hot encoded sparse matrix\n",
    "\n",
    "    # 음식, 실내 ,실외 순으로 들어가기 위해 index 처리\n",
    "    # train_labels = sorted(train_labels)\n",
    "    # print(train_labels)\n",
    "\n",
    "    train_labels = pd.get_dummies((train_labels)).values\n",
    "    # print(train_labels)\n",
    "    # Converting train_images to array\n",
    "    train_images = np.array(train_images)\n",
    "\n",
    "    # Splitting Training data into train and validation dataset\n",
    "\n",
    "    train_images ,train_labels= shuffle(train_images ,train_labels, random_state = 42) \n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(train_images,train_labels,test_size=0.2,random_state=42)\n",
    "    # print(train_labels.shape)\n",
    "    # print(train_labels)\n",
    "    # print(len(x_train),len(x_test),len(y_train),len(y_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "활성함수  \n",
    "1) Conv2D(32, kernel_size=3, activation='selu)  \n",
    "2) Dropout(0.5)  \n",
    "3) MaxPooling2D(pool_size=2)  \n",
    "  \n",
    "4) Conv2D(32, kernel_size=3, activation='selu')  \n",
    "5) MaxPooling2D(pool_size=2)  \n",
    "  \n",
    "6) Dense(20,activation='selu')  \n",
    "7) Flatten()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_model():\n",
    "# Creating a Sequential model\n",
    "    \n",
    "    model = Sequential([ Input(shape=(300,300,3), name='input_layer'),\n",
    "        Conv2D(32, kernel_size=3, activation='selu', name='conv_layer1'),\n",
    "\n",
    "        Dropout(0.5),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        Conv2D(64, kernel_size=3, activation='selu', name='conv_layer2'),\n",
    "        MaxPooling2D(pool_size=2),\n",
    "        #Dense(20, activation='softmax', name='output_layer')\n",
    "        Dense(20,activation='selu',name='FC_layer1'),\n",
    "        Flatten(),\n",
    "        Dense(3, activation='softmax', name='output_layer')\n",
    "            ])\n",
    "\n",
    "    model.compile(optimizer= 'adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    # Model Summary\n",
    "    model.summary()\n",
    "\n",
    "    # Training the model\n",
    "    history = model.fit(X_train,y_train, validation_data=(X_test, y_test), batch_size=50, epochs = 3)\n",
    "    model.save('model-3')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.77 GiB for an array with shape (15000, 300, 300, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b6094fb81dd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mimage_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6a9be672bb0a>\u001b[0m in \u001b[0;36mimage_load_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# print(train_labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Converting train_images to array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Splitting Training data into train and validation dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.77 GiB for an array with shape (15000, 300, 300, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train, y_test =image_load_data()\n",
    "model, history = train_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(history):\n",
    "    plt.figure(figsize = (5,3))\n",
    "    \n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    \n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train','test'], loc = 'upper right')\n",
    "    plt.show\n",
    "    \n",
    "plot_loss_curve(history)\n",
    "print(history.history)\n",
    "print('train loss =', history.history['loss'][-1])\n",
    "print('validation loss =', history.history['val_loss'][-1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_sample(model,X_test, y_test, test_id= -1 ):\n",
    "    if test_id <0 :\n",
    "        from random import randrange\n",
    "        test_sample_id = randrange(10000)\n",
    "    else:\n",
    "        test_sample_id = test_id\n",
    "    test_image =x_test[test_sample_id].reshape(1,300,300,3)\n",
    "#     test_image =test_images[test_sample_id]\n",
    "#     plt.imshow(test_images[123])\n",
    "\n",
    "    y_actual = y_test[test_sample_id].tolist().index(1)\n",
    "    print('y_actual number = ', y_actual)\n",
    "    \n",
    "    y_pred = model.predict(test_image)\n",
    "    print(\"y_pred = \", y_pred)\n",
    "    y_pred_num = np.argmax(y_pred, axis = 1)[0]\n",
    "    print('y_pred number = ', y_pred_num)\n",
    "    return y_actual , y_pred_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.77 GiB for an array with shape (15000, 300, 300, 3) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ee6014f1cffa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mimage_load_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model-4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6a9be672bb0a>\u001b[0m in \u001b[0;36mimage_load_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# print(train_labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Converting train_images to array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Splitting Training data into train and validation dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.77 GiB for an array with shape (15000, 300, 300, 3) and data type uint8"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' :\n",
    "    X_train,X_test,y_train, y_test =image_load_data()\n",
    "    n = 0\n",
    "    model = load_model('model-4')\n",
    "\n",
    "    for i in range(20):\n",
    "        y_actual , y_pred_num = predict_image_sample(model,X_test, y_test,test_id = i)\n",
    "        \n",
    "        if y_actual == y_pred_num:\n",
    "            n += 1 \n",
    "        else : pass\n",
    "print('전체 횟수 :', i,' '*10,'예측 성공 횟수',n,'\\n'*2 ,'정확도 = ', (n/i)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
